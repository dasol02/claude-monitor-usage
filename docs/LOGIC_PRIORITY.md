# ë…¼ë¦¬ ì—°ì‚° ìš°ì„ ìˆœìœ„ ë° ì¡°ê±´ ì •ë¦¬

## ğŸ“‘ ëª©ì°¨

1. [ë””ìŠ¤í”Œë ˆì´ ê°’ ê²°ì • ë¡œì§](#-ë””ìŠ¤í”Œë ˆì´-ê°’-ê²°ì •-ë¡œì§)
   - [Monitor Daemon (monitor_daemon.py)](#1-monitor-daemon-monitor_daemonpy543-565)
   - [Calibration Learner (calibration_learner.py)](#2-calibration-learner-calibration_learnerpy309-392)
   - [SwiftBar Plugin (ClaudeUsage.1m.sh)](#3-swiftbar-plugin-claudeusage1msh47-61)

2. [ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë¸ í•™ìŠµ ì¡°ê±´](#-ìº˜ë¦¬ë¸Œë ˆì´ì…˜-ëª¨ë¸-í•™ìŠµ-ì¡°ê±´)
   - [Model Update í”„ë¡œì„¸ìŠ¤](#model-update-calibration_learnerpy172-306)

3. [Global Fallback Limit ì‹œìŠ¤í…œ](#-global-fallback-limit-ì‹œìŠ¤í…œ)
   - [ì‘ë™ ì›ë¦¬](#ì‘ë™-ì›ë¦¬)
   - [Fallback ìš°ì„ ìˆœìœ„](#fallback-ìš°ì„ ìˆœìœ„)
   - [ê°€ì¤‘ í‰ê·  ê³„ì‚°](#ê°€ì¤‘-í‰ê· -ê³„ì‚°)

4. [ê¶Œì¥ ì¡°ì • ì‹œë‚˜ë¦¬ì˜¤](#-ê¶Œì¥-ì¡°ì •-ì‹œë‚˜ë¦¬ì˜¤)
   - [ì‹œë‚˜ë¦¬ì˜¤ 1: ë¹ ë¥¸ í•™ìŠµ, ë‚®ì€ ì •í™•ë„](#ì‹œë‚˜ë¦¬ì˜¤-1-ë¹ ë¥¸-í•™ìŠµ-ë‚®ì€-ì •í™•ë„)
   - [ì‹œë‚˜ë¦¬ì˜¤ 2: ëŠë¦° í•™ìŠµ, ë†’ì€ ì •í™•ë„](#ì‹œë‚˜ë¦¬ì˜¤-2-ëŠë¦°-í•™ìŠµ-ë†’ì€-ì •í™•ë„)
   - [ì‹œë‚˜ë¦¬ì˜¤ 3: ê· í˜•ì¡íŒ ì„¤ì • (í˜„ì¬ ê¶Œì¥)](#ì‹œë‚˜ë¦¬ì˜¤-3-ê· í˜•ì¡íŒ-ì„¤ì •-í˜„ì¬-ê¶Œì¥)

5. [ìˆ˜ì •ì´ í•„ìš”í•œ ê²½ìš°](#-ìˆ˜ì •ì´-í•„ìš”í•œ-ê²½ìš°)
   - [ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ë³€ê²½](#1-ìµœì†Œ-ìƒ˜í”Œ-ìˆ˜-ë³€ê²½)
   - [Confidence ì„ê³„ê°’ ì¶”ê°€](#2-confidence-ì„ê³„ê°’-ì¶”ê°€)
   - [SwiftBar status/confidence í™•ì¸](#3-swiftbarì—ì„œ-statusconfidence-í™•ì¸)

6. [í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸](#-í…ŒìŠ¤íŠ¸-ì²´í¬ë¦¬ìŠ¤íŠ¸)

7. [í˜„ì¬ ì„¤ì • ìš”ì•½](#í˜„ì¬-ì„¤ì •-ìš”ì•½)

8. [ğŸ†• ìµœê·¼ ê°œì„ ì‚¬í•­](#-ìµœê·¼-ê°œì„ ì‚¬í•­-2025-10-16-2025-10-17)
   - [Override ì‹¤ì‹œê°„ ê³„ì‚° êµ¬í˜„](#override-ì‹¤ì‹œê°„-ê³„ì‚°-êµ¬í˜„-2025-10-16)
   - [Global Fallback Limit ì‹œìŠ¤í…œ](#global-fallback-limit-ì‹œìŠ¤í…œ-êµ¬í˜„-2025-10-17)
   - [Legacy ëª¨ë“ˆ ì œê±°](#legacy-limit_learner-ì œê±°-2025-10-17)

---

## ğŸ“Š ë””ìŠ¤í”Œë ˆì´ ê°’ ê²°ì • ë¡œì§

### 1. Monitor Daemon (monitor_daemon.py:543-565)

**ìš°ì„ ìˆœìœ„:**
```
1. CALIBRATION_ENABLEDê°€ Trueì¸ê°€?
   â””â”€ YES â†’ calibration ê³„ì‚° ì‹œë„
            â”œâ”€ calibration['status'] in ['override', 'calibrated', 'learning']ì¸ê°€?
            â”‚  â””â”€ YES â†’ display_percentage = calibrated_percentage âœ…
            â”‚  â””â”€ NO  â†’ display_percentage = original_percentage
            â””â”€ ê³„ì‚° ì‹¤íŒ¨ (Exception)
                      â†’ display_percentage = original_percentage
   â””â”€ NO  â†’ display_percentage = original_percentage
```

**í˜„ì¬ ì½”ë“œ:**
```python
# Line 545
display_percentage = session_percentages['max_percentage']  # ê¸°ë³¸ê°’

if CALIBRATION_ENABLED:
    try:
        calibration = get_calibrated_value(...)

        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì´ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ë³´ì •ëœ ê°’ì„ ì‚¬ìš©
        # override, calibrated, learning ëª¨ë‘ ì ìš©
        if calibration['status'] in ['override', 'calibrated', 'learning']:
            display_percentage = calibration_info['calibrated_percentage']
    except Exception as e:
        # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ìœ ì§€
```

**ì¡°ì • í¬ì¸íŠ¸:**
- [x] **Override ìƒíƒœ ì¶”ê°€** (2025-10-16)
- [ ] 'learning' ìƒíƒœì—ì„œë„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ì ìš©í•  ê²ƒì¸ê°€?
- [ ] confidence ì„ê³„ê°’ì„ ì¶”ê°€í•  ê²ƒì¸ê°€? (ì˜ˆ: confidence >= 0.5ì¼ ë•Œë§Œ ì ìš©)
- [ ] sample_count ìµœì†Œê°’ì„ í™•ì¸í•  ê²ƒì¸ê°€?

---

### 2. Calibration Learner (calibration_learner.py:388-652)

**ìš°ì„ ìˆœìœ„:**
```
1. Override í™•ì¸ (ìµœìš°ì„ )
   â””â”€ latest_overrideê°€ ìˆê³  ë§Œë£Œë˜ì§€ ì•Šì•˜ëŠ”ê°€?
      â”œâ”€ YES + learned_limit > 0
      â”‚  â””â”€ ì‹¤ì‹œê°„ í† í°ìœ¼ë¡œ percentage ê³„ì‚° âœ… (override_limit_based)
      â”œâ”€ YES + learned_limit ì—†ìŒ
      â”‚  â””â”€ ê³ ì •ê°’ ì‚¬ìš© (override_fixed)
      â””â”€ NO â†“

2. window_keyì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ì´ ìˆëŠ”ê°€?
   â””â”€ NO â†’ status='no_data', offset=0, ì›ë³¸ ê°’ ë°˜í™˜ âŒ
   â””â”€ YES â†“

3. sample_count >= 3 ì¸ê°€?
   â””â”€ NO â†’ Global fallback limit ì‹œë„ âš ï¸
      â”œâ”€ fallback_limit ìˆìŒ
      â”‚  â””â”€ ì‹¤ì‹œê°„ í† í°ìœ¼ë¡œ percentage ê³„ì‚° âœ… (learning_with_fallback)
      â””â”€ fallback_limit ì—†ìŒ
         â””â”€ ì›ë³¸ ê°’ ë°˜í™˜ âŒ
   â””â”€ YES â†“

4. modelì— limit learningì´ ìˆëŠ”ê°€? (has_limit_learning)
   â”œâ”€ YES â†’ limit ê¸°ë°˜ ì‹¤ì‹œê°„ ê³„ì‚° ì‹œë„
   â”‚        â”œâ”€ ì„±ê³µ â†’ calibrated_value (limit_based) âœ…
   â”‚        â””â”€ ì‹¤íŒ¨ â†’ calibrated_value (offset_fallback) âœ…
   â””â”€ NO  â†’ Global fallback limit ì‹œë„ âš ï¸
      â”œâ”€ fallback_limit ìˆìŒ
      â”‚  â””â”€ ì‹¤ì‹œê°„ í† í°ìœ¼ë¡œ percentage ê³„ì‚° âœ… (fallback_limit)
      â””â”€ fallback_limit ì—†ìŒ
         â””â”€ calibrated_value (offset_based) âœ…
```

**í˜„ì¬ ì½”ë“œ:**
```python
# Line 401-450: Override í™•ì¸ (ìµœìš°ì„ )
if window_key in data and 'latest_override' in data[window_key]:
    override = data[window_key]['latest_override']
    if datetime.now(ZoneInfo('Asia/Seoul')) < expires_at:
        learned_limit = override.get('learned_limit')

        if learned_limit and learned_limit > 0:
            # ì‹¤ì‹œê°„ í† í° ê³„ì‚°
            output_total = usage_data['session']['usage']['output_tokens']
            calibrated_pct = (output_total / (learned_limit * 300)) * 100
            method = 'override_limit_based'
        else:
            # ê³ ì •ê°’ ì‚¬ìš©
            calibrated_value = override['calibrated_percentage'] / 100
            method = 'override_fixed'

# Line 452-520: ëª¨ë¸ ê¸°ë°˜ ê³„ì‚°
if model.get('has_limit_learning'):
    # Limit ê¸°ë°˜ ì˜ˆì¸¡ (ë” ì •í™•í•¨)
    try:
        calibrated_value = ...  # limit ê¸°ë°˜
        method = 'limit_based'
    except:
        calibrated_value = monitor_value + model['offset_mean']
        method = 'offset_fallback'
else:
    # Offset ê¸°ë°˜ ì˜ˆì¸¡ (ì´ˆê¸° í•™ìŠµ)
    calibrated_value = monitor_value + model['offset_mean']
    method = 'offset_based'
```

**ì¡°ì • í¬ì¸íŠ¸:**
- [x] **Override ì‹¤ì‹œê°„ ê³„ì‚° êµ¬í˜„** (2025-10-16)
- [ ] sample_count ìµœì†Œê°’ (í˜„ì¬: 3) â†’ ì¡°ì • ê°€ëŠ¥
- [ ] confidence ì„ê³„ê°’ ì¶”ê°€? (ì˜ˆ: confidence >= 0.6)
- [ ] learning ìƒíƒœì—ì„œë„ ì ìš©? (í˜„ì¬: YES)
- [ ] limit_based ì‹¤íŒ¨ì‹œ fallback ë¡œì§

---

### 3. SwiftBar Plugin (ClaudeUsage.1m.sh:47-61)

**ìš°ì„ ìˆœìœ„:**
```
1. CALIBRATION_ENABLED == "true"ì¸ê°€?
   â””â”€ NO â†’ SESSION_PCT = original âŒ
   â””â”€ YES â†“

2. calibration.info.calibrated_percentageê°€ nullì´ ì•„ë‹Œê°€?
   â””â”€ NO â†’ SESSION_PCT = original âŒ
   â””â”€ YES â†’ SESSION_PCT = calibrated âœ…
```

**í˜„ì¬ ì½”ë“œ:**
```bash
# Line 48
CALIBRATION_ENABLED=$(jq -r '.calibration.enabled // false' "$USAGE_FILE")
if [[ "$CALIBRATION_ENABLED" == "true" ]]; then
    CALIBRATED_PCT=$(jq -r '.calibration.info.calibrated_percentage // null' "$USAGE_FILE")
    if [[ "$CALIBRATED_PCT" != "null" ]]; then
        SESSION_PCT="$CALIBRATED_PCT"
    else
        SESSION_PCT=$(jq -r '.session.percentages.max_percentage // 0' "$USAGE_FILE")
    fi
else
    SESSION_PCT=$(jq -r '.session.percentages.max_percentage // 0' "$USAGE_FILE")
fi
```

**ì¡°ì • í¬ì¸íŠ¸:**
- [ ] calibration statusë„ í™•ì¸í•  ê²ƒì¸ê°€? (í˜„ì¬: NO)
- [ ] confidenceë„ í™•ì¸í•  ê²ƒì¸ê°€? (í˜„ì¬: NO)

---

## ğŸ¯ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë¸ í•™ìŠµ ì¡°ê±´

### Model Update (calibration_learner.py:251-385)

**ìš°ì„ ìˆœìœ„:**
```
1. historyê°€ ìˆëŠ”ê°€?
   â””â”€ NO â†’ status='no_data', ë¹ˆ ëª¨ë¸ ë°˜í™˜ âŒ
   â””â”€ YES â†“

2. len(history) >= 3 ì¸ê°€?
   â””â”€ NO â†’ status='insufficient_data', offset=0 ëª¨ë¸ ë°˜í™˜ âš ï¸
   â””â”€ YES â†“

3. ìƒ˜í”Œ ìˆ˜ì— ë”°ë¼ recent_history ì„ íƒ:
   â”œâ”€ <= 10ê°œ â†’ ìµœê·¼ 5ê°œ ì‚¬ìš©
   â”œâ”€ <= 30ê°œ â†’ ìµœê·¼ 20ê°œ ì‚¬ìš©
   â””â”€ > 30ê°œ  â†’ ìµœê·¼ 50ê°œ ì‚¬ìš©

4. Limit í•™ìŠµ (í† í° ë°ì´í„°ê°€ ìˆëŠ” ìƒ˜í”Œ >= 3ê°œ):
   - Input limit: input_tokens + cache_creation_tokens ê¸°ì¤€ ì—­ì‚°
   - Output limit: output_tokens ê¸°ì¤€ ì—­ì‚°
   - Exponential weighted average (decay_factor = 0.85)

5. offset_mean ê³„ì‚° (exponential weighted average)
   - decay_factor = 0.9 (ìµœì‹ ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ë†’ìŒ)

6. confidence ê³„ì‚°:
   - sample_confidence = min(sample_count / target_samples, 1.0)
   - stability_confidence = max(0.0, 1.0 - offset_std * 10)
   - confidence = (sample_confidence + stability_confidence) / 2.0

7. status ê²°ì •:
   â”œâ”€ confidence < 0.7 â†’ status='learning'
   â””â”€ confidence >= 0.7 â†’ status='learned'
```

**Limit ì—­ì‚° ê³µì‹:**
```
limit_tpm = current_tokens / (actual_percentage / 100) / window_minutes

ì˜ˆì‹œ:
- Output tokens: 41,552
- Actual percentage: 9%
- Window: 300ë¶„ (5ì‹œê°„)
â†’ learned_output_limit = 41,552 / 0.09 / 300 = 1,539 TPM
```

**ì¡°ì • í¬ì¸íŠ¸:**
- [x] **Limit í•™ìŠµ ë° ì‹¤ì‹œê°„ ê³„ì‚°** (2025-10-16)
- [ ] ìµœì†Œ ìƒ˜í”Œ ìˆ˜: 3ê°œ â†’ ?ê°œ
- [ ] decay_factor: 0.9 â†’ ?
- [ ] confidence ì„ê³„ê°’: 0.7 â†’ ?
- [ ] recent_history ìƒ˜í”Œ ìˆ˜ ì¡°ì •

---

## ğŸ”„ Global Fallback Limit ì‹œìŠ¤í…œ

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-17

### ì‘ë™ ì›ë¦¬

ìƒˆ ì„¸ì…˜ ë˜ëŠ” ë°ì´í„° ë¶€ì¡±(ìƒ˜í”Œ < 3ê°œ) ì‹œ, ë‹¤ë¥¸ ì„¸ì…˜ì˜ learned limitì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ fallback limitìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

**í•µì‹¬ ê°œë…**:
- ëª¨ë“  ì„¸ì…˜ì€ ë™ì¼í•œ API limit ê³µìœ  (ì˜ˆ: Output 1,611 TPM)
- ì„¸ì…˜ ì‹œê°„ëŒ€ëŠ” ë‹¤ë¥´ì§€ë§Œ limitì€ ë™ì¼
- ë‹¤ë¥¸ ì„¸ì…˜ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ì´ˆê¸° ê¸°ì¤€ìœ¼ë¡œ í™œìš©
- ê° ì„¸ì…˜ì€ ë…ë¦½ì ìœ¼ë¡œ ê³„ì† í•™ìŠµ

### Fallback ìš°ì„ ìˆœìœ„

```python
# calibration_learner.py:566-604
if model['sample_count'] < 3:
    # 1. Global fallback limit ì‹œë„
    fallback_limit = get_global_fallback_limit()

    if fallback_limit and fallback_limit > 0 and window_key != "weekly":
        # Fallback limitìœ¼ë¡œ ì‹¤ì‹œê°„ ê³„ì‚°
        output_total = usage_data['session']['usage']['output_tokens']
        calibrated_pct = (output_total / (fallback_limit * 300)) * 100

        return {
            'status': 'learning_with_fallback',
            'method': 'fallback_limit',
            'fallback_limit': fallback_limit,
            'confidence': 0.5  # ì¤‘ê°„ ì‹ ë¢°ë„
        }
    else:
        # Fallback ì‹¤íŒ¨ ì‹œ ì›ë³¸ê°’ ì‚¬ìš©
        return {
            'status': 'insufficient_data',
            'calibrated_value': monitor_value
        }
```

### ê°€ì¤‘ í‰ê·  ê³„ì‚°

```python
# calibration_learner.py:205-243
def get_global_fallback_limit() -> Optional[int]:
    """
    ëª¨ë“  ì„¸ì…˜ì˜ learned_output_limitì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ fallback limit ê³„ì‚°
    """
    data = load_calibration_data()

    limits = []
    weights = []

    # ëª¨ë“  ì„¸ì…˜ ìœˆë„ìš°ì—ì„œ learned_output_limit ìˆ˜ì§‘ (weekly ì œì™¸)
    for window_key, window_data in data.items():
        if window_key == "weekly":
            continue

        model = window_data.get('model')
        if not model:
            continue

        learned_limit = model.get('learned_output_limit')
        sample_count = model.get('sample_count', 0)

        # Learned limitì´ ìˆê³  ìƒ˜í”Œì´ 3ê°œ ì´ìƒì¸ ê²½ìš°ë§Œ ì‚¬ìš©
        if learned_limit and learned_limit > 0 and sample_count >= 3:
            limits.append(learned_limit)
            # ìƒ˜í”Œ ìˆ˜ì— ë¹„ë¡€í•œ ê°€ì¤‘ì¹˜ (ë” ë§ì€ ìƒ˜í”Œ = ë” ë†’ì€ ì‹ ë¢°ë„)
            weight = min(sample_count / 10.0, 1.0)  # ìµœëŒ€ 1.0
            weights.append(weight)

    if not limits:
        return None

    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
    total_weight = sum(weights)
    weighted_avg = sum(l * w for l, w in zip(limits, weights)) / total_weight

    return round(weighted_avg)
```

### ì‹¤ì œ ì˜ˆì‹œ

**ìƒí™©**: ìƒˆë¡œìš´ ì„¸ì…˜ 09:00-14:00 (ìƒ˜í”Œ 2ê°œ)

**ë‹¤ë¥¸ ì„¸ì…˜ ë°ì´í„°**:
- 14:00-19:00: 29 ìƒ˜í”Œ, learned_limit = 1,293 TPM
- 05:00-10:00: 1 ìƒ˜í”Œ (ë¬´ì‹œ, < 3ê°œ)

**Fallback ê³„ì‚°**:
```python
# 14:00-19:00 ì„¸ì…˜ë§Œ ì‚¬ìš© (3ê°œ ì´ìƒ)
limits = [1293]
weights = [min(29 / 10.0, 1.0)] = [1.0]  # ìµœëŒ€ ê°€ì¤‘ì¹˜

fallback_limit = 1293 * 1.0 / 1.0 = 1,293 TPM
```

**09:00-14:00 ì„¸ì…˜ í¼ì„¼íŠ¸ ê³„ì‚°**:
```python
# Output tokens: 6,408
# Fallback limit: 1,293 TPM
# Window: 300ë¶„ (5ì‹œê°„)

percentage = (6408 / (1293 * 300)) * 100 = 1.7%

# Status: 'learning_with_fallback'
# Confidence: 0.5
```

### Fallback ì ìš© ì¡°ê±´

**ì ìš©ë¨**:
1. ì„¸ì…˜ ìƒ˜í”Œ < 3ê°œ
2. ì„¸ì…˜ì— limit learning ì—†ìŒ (has_limit_learning = False)
3. Weekly ìœˆë„ìš° ì œì™¸ (ì„¸ì…˜ë§Œ)

**ì ìš© ì•ˆë¨**:
1. Override í™œì„±í™” (ìµœìš°ì„ )
2. ìƒ˜í”Œ >= 3ê°œ + limit learning ìˆìŒ
3. Weekly ìœˆë„ìš°

### ìƒíƒœ (Status)

**`learning_with_fallback`**:
- ìƒ˜í”Œ < 3ê°œì´ì§€ë§Œ fallback limitìœ¼ë¡œ ê³„ì‚°
- Confidence: 0.5 (ì¤‘ê°„ ì‹ ë¢°ë„)
- Method: `fallback_limit`
- ê° ì„¸ì…˜ì€ ê³„ì† ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ
- ìƒ˜í”Œ 3ê°œ ì´ìƒ ì‹œ ìì²´ learned_limit ì‚¬ìš©

---

## ğŸ”§ ê¶Œì¥ ì¡°ì • ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë¹ ë¥¸ í•™ìŠµ, ë‚®ì€ ì •í™•ë„
```
calibration_learner.py:
  - sample_count ìµœì†Œê°’: 3 â†’ 2
  - confidence ì„ê³„ê°’: 0.7 â†’ 0.5

monitor_daemon.py:
  - 'learning' ìƒíƒœì—ì„œë„ ì ìš©: YES (í˜„ì¬ ì„¤ì • ìœ ì§€)
  - confidence í™•ì¸ ì—†ìŒ (í˜„ì¬ ì„¤ì • ìœ ì§€)
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ëŠë¦° í•™ìŠµ, ë†’ì€ ì •í™•ë„
```
calibration_learner.py:
  - sample_count ìµœì†Œê°’: 3 â†’ 5
  - confidence ì„ê³„ê°’: 0.7 â†’ 0.8

monitor_daemon.py:
  - 'learning' ìƒíƒœ ì œì™¸: ['override', 'calibrated'] ë§Œ ì ìš©
  - ë˜ëŠ” confidence >= 0.7 ì¡°ê±´ ì¶”ê°€
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ê· í˜•ì¡íŒ ì„¤ì • (í˜„ì¬ ê¶Œì¥)
```
calibration_learner.py:
  - sample_count ìµœì†Œê°’: 3 (í˜„ì¬ ì„¤ì •)
  - confidence ì„ê³„ê°’: 0.7 (í˜„ì¬ ì„¤ì •)
  - Override ì‹¤ì‹œê°„ ê³„ì‚°: YES âœ… (2025-10-16)

monitor_daemon.py:
  - 'learning' ìƒíƒœì—ì„œë„ ì ìš©: YES
  - 'override' ìƒíƒœ ìµœìš°ì„  ì ìš©: YES âœ… (2025-10-16)
  - confidence í™•ì¸: NO (ì‹ ë¢°ë„ ë‚®ì•„ë„ ì ìš©)
```

---

## ğŸ“ ìˆ˜ì •ì´ í•„ìš”í•œ ê²½ìš°

### 1. ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ë³€ê²½

**íŒŒì¼:** `src/calibration_learner.py`

```python
# Line 278: insufficient_data ì¡°ê±´
if len(history) < 3:  # â† ì´ ìˆ«ì ë³€ê²½

# Line 440: calibrated_value ë°˜í™˜ ì¡°ê±´
if model['sample_count'] < 3:  # â† ì´ ìˆ«ì ë³€ê²½
```

### 2. Confidence ì„ê³„ê°’ ì¶”ê°€

**íŒŒì¼:** `src/monitor_daemon.py`

```python
# Line 565: ê¸°ì¡´
if calibration['status'] in ['override', 'calibrated', 'learning']:
    display_percentage = calibration_info['calibrated_percentage']

# ìˆ˜ì •ì•ˆ 1: statusë§Œ ì œí•œ
if calibration['status'] in ['override', 'calibrated']:  # learning ì œì™¸
    display_percentage = calibration_info['calibrated_percentage']

# ìˆ˜ì •ì•ˆ 2: confidence ì¶”ê°€ (overrideëŠ” ì œì™¸)
if calibration['status'] == 'override':
    display_percentage = calibration_info['calibrated_percentage']
elif calibration['status'] in ['calibrated', 'learning'] and calibration['confidence'] >= 0.6:
    display_percentage = calibration_info['calibrated_percentage']
```

### 3. SwiftBarì—ì„œ status/confidence í™•ì¸

**íŒŒì¼:** `plugins/ClaudeUsage.1m.sh`

```bash
# Line 49 ì´í›„ ì¶”ê°€:
if [[ "$CALIBRATION_ENABLED" == "true" ]]; then
    CALIBRATED_PCT=$(jq -r '.calibration.info.calibrated_percentage // null' "$USAGE_FILE")
    CALIBRATION_STATUS=$(jq -r '.calibration.info.status // "no_data"' "$USAGE_FILE")
    CONFIDENCE=$(jq -r '.calibration.info.confidence // 0' "$USAGE_FILE")

    # ì¡°ê±´ ì¶”ê°€ (overrideëŠ” í•­ìƒ ì ìš©)
    if [[ "$CALIBRATED_PCT" != "null" ]] && \
       ([[ "$CALIBRATION_STATUS" == "override" ]] || [[ "$CALIBRATION_STATUS" == "calibrated" ]]); then
        SESSION_PCT="$CALIBRATED_PCT"
    else
        SESSION_PCT=$(jq -r '.session.percentages.max_percentage // 0' "$USAGE_FILE")
    fi
fi
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

ë³€ê²½ í›„ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:

- [ ] `python3 src/calibration_learner.py --status` - ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ìƒíƒœ í™•ì¸
- [ ] `python3 src/monitor_daemon.py --once` - ëª¨ë‹ˆí„° í•œ ë²ˆ ì‹¤í–‰
- [ ] JSON ì¶œë ¥ì—ì„œ `calibration.info` í™•ì¸
- [ ] SwiftBar ë©”ë‰´ë°”ì—ì„œ í‘œì‹œê°’ í™•ì¸
- [ ] ìƒˆë¡œìš´ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ì¶”ê°€ í›„ ì¬í™•ì¸
- [x] **Override ì‹¤ì‹œê°„ ê³„ì‚° í…ŒìŠ¤íŠ¸** (2025-10-16)
  - [x] `claude-calibrate 9` ì…ë ¥
  - [x] í† í° ì¦ê°€ ì‹œ % ì¦ê°€ í™•ì¸

---

## í˜„ì¬ ì„¤ì • ìš”ì•½

| í•­ëª© | í˜„ì¬ ê°’ | ìœ„ì¹˜ | ì¡°ì • ê°€ëŠ¥? | ë¹„ê³  |
|------|---------|------|-----------|------|
| ìµœì†Œ ìƒ˜í”Œ ìˆ˜ | 3 | calibration_learner.py:278,440 | âœ… | |
| Confidence ì„ê³„ê°’ (í•™ìŠµ) | 0.7 | calibration_learner.py:374 | âœ… | |
| Display ì ìš© status | 'override', 'calibrated', 'learning' | monitor_daemon.py:565 | âœ… | Override ì¶”ê°€ (2025-10-16) |
| Display confidence í™•ì¸ | NO | monitor_daemon.py:565 | âœ… ì¶”ê°€ ê°€ëŠ¥ | OverrideëŠ” ì œì™¸ |
| SwiftBar status í™•ì¸ | NO | ClaudeUsage.1m.sh:49 | âœ… ì¶”ê°€ ê°€ëŠ¥ | |
| Decay factor (offset) | 0.9 | calibration_learner.py:348 | âœ… | |
| Decay factor (limit) | 0.85 | calibration_learner.py:336 | âœ… | |
| Recent samples (ì´ˆê¸°) | 5 | calibration_learner.py:298 | âœ… | |
| **Override ì‹¤ì‹œê°„ ê³„ì‚°** | **YES** | calibration_learner.py:410-425 | âœ… | **ì‹ ê·œ (2025-10-16)** |
| **Override limit ì €ì¥** | **Output only** | calibration_learner.py:719-723 | âœ… | **ê°œì„  (2025-10-16)** |
| **Global Fallback Limit** | **YES** | calibration_learner.py:205-243, 566-604 | âœ… | **ì‹ ê·œ (2025-10-17)** |
| **Fallback ìµœì†Œ ìƒ˜í”Œ** | **3** | calibration_learner.py:220 | âœ… | **ì„¸ì…˜ ë°ì´í„° 3ê°œ ì´ìƒë§Œ ì‚¬ìš©** |

---

## ğŸ†• ìµœê·¼ ê°œì„ ì‚¬í•­ (2025-10-16 ~ 2025-10-17)

### Override ì‹¤ì‹œê°„ ê³„ì‚° êµ¬í˜„ (2025-10-16)

### ë¬¸ì œ ìƒí™©

**ì‚¬ìš©ì ìš”ì²­:**
```
í˜„ì¬ ëª¨ë‹ˆí„° 7% â†’ claude-calibrate 10 ì…ë ¥
â†’ ëª¨ë‹ˆí„° 10% ë…¸ì¶œ (ì¦‰ì‹œ ë°˜ì˜ âœ…)
â†’ 1ë¶„ í›„ ì›ë˜ ê°’ 7%ë¡œ ë³µì› âŒ
```

**ì›ì¸ ë¶„ì„:**
1. Overrideê°€ **ê³ ì •ê°’**ì„ ì €ì¥í•˜ê³  ë°˜í™˜
2. í† í°ì´ ê³„ì† ì¦ê°€í•´ë„ 10%ë¡œ ê³ ì •
3. í•™ìŠµëœ limitì´ ìˆì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

**ì¶”ê°€ ë¬¸ì œ:**
- `max(input_limit, output_limit)` ì €ì¥ â†’ Input limit (í° ê°’) ì €ì¥
- Output limitìœ¼ë¡œ ê³„ì‚° ì‹œ ê³¼ë„í•˜ê²Œ ë‚®ì€ % í‘œì‹œ
- ì˜ˆ: Output 41,552 tokens, Limit 23,857 TPM â†’ **0.6%** (ì˜ëª»ë¨)

---

### í•´ê²° ë°©ë²•

#### 1. Override ì‹¤ì‹œê°„ ê³„ì‚° êµ¬í˜„

**íŒŒì¼:** `src/calibration_learner.py:401-450`

**Before (ê³ ì •ê°’ ë°˜í™˜):**
```python
if window_key in data and 'latest_override' in data[window_key]:
    override = data[window_key]['latest_override']
    if datetime.now(ZoneInfo('Asia/Seoul')) < expires_at:
        # âŒ ê³ ì •ê°’ë§Œ ë°˜í™˜
        return {
            'calibrated_value': override['calibrated_percentage'] / 100,
            'status': 'override',
            'method': 'manual_override'
        }
```

**After (ì‹¤ì‹œê°„ ê³„ì‚°):**
```python
if window_key in data and 'latest_override' in data[window_key]:
    override = data[window_key]['latest_override']
    if datetime.now(ZoneInfo('Asia/Seoul')) < expires_at:
        learned_limit = override.get('learned_limit')

        if learned_limit and learned_limit > 0:
            # âœ… ì‹¤ì‹œê°„ í† í°ìœ¼ë¡œ percentage ê³„ì‚°
            output_file = Path.home() / '.claude_usage.json'
            with open(output_file, 'r') as f:
                usage_data = json.load(f)

            output_total = usage_data['session']['usage']['output_tokens']
            calibrated_pct = (output_total / (learned_limit * 300)) * 100
            calibrated_value = calibrated_pct / 100.0
            method = 'override_limit_based'
        else:
            # Learned limit ì—†ìœ¼ë©´ ê³ ì •ê°’ ì‚¬ìš©
            calibrated_value = override['calibrated_percentage'] / 100
            method = 'override_fixed'

        return {
            'calibrated_value': calibrated_value,
            'status': 'override',
            'method': method,
            'learned_limit': learned_limit
        }
```

---

#### 2. Output Limit ì „ìš© ì‚¬ìš©

**íŒŒì¼:** `src/calibration_learner.py:717-724`

**Before (max ì‚¬ìš©):**
```python
# Inputê³¼ Output ì¤‘ í° ê°’ ì €ì¥
max_limit = max(learned_input_limit or 0, learned_output_limit or 0)
set_calibration_override(
    window_key,
    session_actual,
    max_limit,  # âŒ Input limit (í° ê°’) ì €ì¥ë¨
    window_end.isoformat()
)
```

**After (Outputë§Œ ì‚¬ìš©):**
```python
# Output limitë§Œ ì €ì¥ (ì‹¤ì œ ê³„ì‚°ì— ì‚¬ìš©í•˜ëŠ” ê°’)
set_calibration_override(
    window_key,
    session_actual,
    learned_output_limit or 0,  # âœ… Output limitë§Œ ì €ì¥
    window_end.isoformat()
)
```

---

### ê°œì„  íš¨ê³¼

**Before:**
```
1. claude-calibrate 10 ì…ë ¥
   â†’ Override: calibrated_percentage = 10% (ê³ ì •)
   â†’ learned_limit = 23,857 (input limit)

2. ëª¨ë‹ˆí„° ì‹¤í–‰ (1ë¶„ í›„)
   â†’ Override ë°˜í™˜: 10% (ê³ ì •ê°’)
   âŒ í† í° ì¦ê°€í•´ë„ 10%ë¡œ ê³ ì •

3. ë˜ëŠ” learned_limitìœ¼ë¡œ ê³„ì‚° ì‹œ:
   â†’ 41,552 / (23,857 * 300) = 0.6%
   âŒ ê³¼ë„í•˜ê²Œ ë‚®ìŒ (Input limit ì‚¬ìš©)
```

**After:**
```
1. claude-calibrate 10 ì…ë ¥
   â†’ Override: calibrated_percentage = 10%
   â†’ learned_limit = 1,539 (output limit)
   â†’ History ê¸°ë¡ (í•™ìŠµìš©)

2. ëª¨ë‹ˆí„° ì‹¤í–‰ (ì¦‰ì‹œ)
   â†’ Output: 41,552 tokens
   â†’ 10.0% = 41,552 / (1,539 * 300) * 100
   âœ… ì‹¤ì‹œê°„ ê³„ì‚°

3. ëª¨ë‹ˆí„° ì‹¤í–‰ (1ë¶„ í›„)
   â†’ Output: 45,000 tokens (ì¦ê°€)
   â†’ 10.8% = 45,000 / (1,539 * 300) * 100
   âœ… í† í° ì¦ê°€ì— ë”°ë¼ % ì¦ê°€

4. ì„¸ì…˜ ë (19:00)
   â†’ Override ë§Œë£Œ
   â†’ ëª¨ë¸ì˜ learned_output_limit ì‚¬ìš©
   âœ… í•™ìŠµëœ ë°ì´í„°ë¡œ ê³„ì† ì •í™•í•œ ê³„ì‚°
```

---

### ë°ì´í„° êµ¬ì¡° ë³€ê²½

**calibration_data.json:**
```json
{
  "14:00-19:00": {
    "latest_override": {
      "timestamp": "2025-10-16T14:45:00+09:00",
      "calibrated_percentage": 10.0,
      "expires_at": "2025-10-16T19:00:00+09:00",
      "learned_limit": 1539  // âœ… Output limitë§Œ ì €ì¥ (ì´ì „: 23857)
    },
    "history": [
      {
        "timestamp": "2025-10-16T14:45:00+09:00",
        "monitor_value": 0.093,
        "actual_value": 0.10,
        "offset": 0.007,
        "token_data": {
          "input_tokens": 1686,
          "output_tokens": 41552,
          "cache_creation_tokens": 730580,
          "total_counted_tokens": 773818
        }
      }
    ],
    "model": {
      "learned_input_limit": 24397,  // ì°¸ê³ ìš©
      "learned_output_limit": 1539,  // âœ… ì‹¤ì œ ì‚¬ìš©
      "has_limit_learning": true,
      "status": "learned"
    }
  }
}
```

---

### ì‚¬ìš© ë°©ë²•

#### ê¸°ë³¸ ì‚¬ìš©
```bash
# 1. Claude usage UIì—ì„œ ì •í™•í•œ % í™•ì¸
/usage
# Output: Session Output: 10%

# 2. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì¦‰ì‹œ ë°˜ì˜ + Limit í•™ìŠµ)
claude-calibrate 10

# ì¶œë ¥:
# âœ… ì¦‰ì‹œ ë°˜ì˜: 10.0% â†’ SwiftBarì— í‘œì‹œë¨
#
# ğŸ“Š Calibration for 14:00-19:00:
#    Monitor: 9.3%
#    Actual:  10.0%
#    Offset:  +0.7%
#
# ğŸ¯ í•™ìŠµëœ Limit:
#    Input:  24,397 TPM
#    Output: 1,539 TPM
#
# â° Override:
#    ë§Œë£Œ ì‹œê°„: 19:00 (2025-10-16)
#    ìƒíƒœ: ì„¸ì…˜ ëë‚  ë•Œê¹Œì§€ ì‹¤ì‹œê°„ ê³„ì‚° ì ìš©
```

#### ë™ì‘ í™•ì¸
```bash
# ëª¨ë‹ˆí„° ì‹¤í–‰ (ì‹¤ì‹œê°„ ê³„ì‚°)
python3 src/monitor_daemon.py --once | jq '.calibration.info'

# ì¶œë ¥:
# {
#   "original_percentage": 9.5,
#   "calibrated_percentage": 10.2,  // âœ… í† í° ì¦ê°€ì— ë”°ë¼ ì¦ê°€
#   "status": "override",
#   "method": "override_limit_based",  // âœ… Limit ê¸°ë°˜ ì‹¤ì‹œê°„ ê³„ì‚°
#   "learned_limit": 1539
# }
```

#### ì„¸ì…˜ ì¢…ë£Œ í›„
```bash
# 19:00 ì´í›„ ìë™ìœ¼ë¡œ Override ë§Œë£Œ
# ëª¨ë¸ì˜ learned_output_limit ì‚¬ìš©í•˜ì—¬ ê³„ì† ì •í™•í•œ ê³„ì‚° ìœ ì§€
```

---

### ì£¼ì˜ì‚¬í•­

1. **Override ë§Œë£Œ**
   - OverrideëŠ” í˜„ì¬ ì„¸ì…˜ ì¢…ë£Œ ì‹œê°ê¹Œì§€ë§Œ ìœ íš¨
   - ë‹¤ìŒ ì„¸ì…˜(19:00-00:00)ì—ì„œëŠ” ëª¨ë¸ ê¸°ë°˜ ê³„ì‚° ì‚¬ìš©
   - í•„ìš” ì‹œ ìƒˆ ì„¸ì…˜ì—ì„œ ì¬ì¡°ì •

2. **Limit ì—­ì‚° ì •í™•ë„**
   - ì‹¤ì œ ì‚¬ìš©ëŸ‰ì´ ë‚®ì„ ë•Œ(< 5%) ì—­ì‚°ëœ limitì˜ ì˜¤ì°¨ ì¦ê°€
   - ê°€ëŠ¥í•œ 10% ì´ìƒì—ì„œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê¶Œì¥
   - ì—¬ëŸ¬ ë²ˆ ì…ë ¥í•˜ì—¬ exponential weighted averageë¡œ ì •í™•ë„ í–¥ìƒ

3. **Input vs Output Limit**
   - Input limit: input_tokens + cache_creation_tokens ê¸°ì¤€
   - Output limit: output_tokens ê¸°ì¤€ (ì‹¤ì œ ë””ìŠ¤í”Œë ˆì´ì— ì‚¬ìš©)
   - ë‘ ê°’ ëª¨ë‘ í•™ìŠµí•˜ì§€ë§Œ OverrideëŠ” **Output limitë§Œ ì‚¬ìš©**

4. **Method í™•ì¸**
   - `override_limit_based`: ì‹¤ì‹œê°„ ê³„ì‚° âœ…
   - `override_fixed`: ê³ ì •ê°’ ì‚¬ìš© (learned_limit ì—†ì„ ë•Œ)
   - `limit_based`: ëª¨ë¸ì˜ learned_limit ì‚¬ìš©
   - `offset_based`: Offset ë³´ì • (ì´ˆê¸° í•™ìŠµ)

---

### í…ŒìŠ¤íŠ¸ ê²°ê³¼

**í…ŒìŠ¤íŠ¸ 1: ì‹¤ì‹œê°„ ê³„ì‚° í™•ì¸**
```bash
# Before calibration
cat ~/.claude_usage.json | jq '.calibration.info.calibrated_percentage'
# â†’ 7.2

# Calibrate
claude-calibrate 9
# â†’ âœ… ì¦‰ì‹œ ë°˜ì˜: 9.0%

# Immediately after
cat ~/.claude_usage.json | jq '.calibration.info'
# â†’ calibrated_percentage: 9.0
# â†’ method: "override_limit_based" âœ…

# 1 minute later (after more tokens)
~/.local/bin/claude-usage-monitor --once
cat ~/.claude_usage.json | jq '.calibration.info.calibrated_percentage'
# â†’ 9.2 âœ… (ì¦ê°€í•¨)
```

**í…ŒìŠ¤íŠ¸ 2: Limit ì €ì¥ í™•ì¸**
```bash
# Check learned limit
cat ~/.claude-monitor/calibration_data.json | jq '.["14:00-19:00"].latest_override.learned_limit'
# â†’ 1712 âœ… (Output limit, ì´ì „: 23857)

# Verify calculation
# Output: 41,552 tokens
# 41,552 / (1712 * 300) * 100 = 8.1% âœ… (ì •ìƒ)
```

---

### Global Fallback Limit ì‹œìŠ¤í…œ êµ¬í˜„ (2025-10-17)

**ë¬¸ì œ ìƒí™©:**
```
ìƒˆë¡œìš´ ì„¸ì…˜ 09:00-14:00 ì‹œì‘
â†’ ìƒ˜í”Œ 0ê°œ (ë˜ëŠ” < 3ê°œ)
â†’ Learned limit ì—†ìŒ
â†’ Config limit ì‚¬ìš© (ë¶€ì •í™•)
â†’ ì‹¤ì œ 35%ì¸ë° 28% í‘œì‹œ âŒ
```

**ì›ì¸ ë¶„ì„:**
- ê° ì„¸ì…˜ ìœˆë„ìš°ê°€ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ìš´ì˜
- ëª¨ë“  ì„¸ì…˜ì€ ë™ì¼í•œ API limit ì‚¬ìš© (ì˜ˆ: Output 1,611 TPM)
- 14:00-19:00 ì„¸ì…˜: 29 ìƒ˜í”Œ, learned_limit = 1,293 TPM (ì •í™•)
- 09:00-14:00 ì„¸ì…˜: 2 ìƒ˜í”Œ, learned_limit ì—†ìŒ (ë¶€ì •í™•)
- ë‹¤ë¥¸ ì„¸ì…˜ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ í™œìš©í•˜ì§€ ëª»í•¨

**í•´ê²° ë°©ë²•:**

#### 1. Global Fallback Limit í•¨ìˆ˜ ì¶”ê°€

**íŒŒì¼**: `src/calibration_learner.py:205-243`

```python
def get_global_fallback_limit() -> Optional[int]:
    """
    ëª¨ë“  ì„¸ì…˜ì˜ learned_output_limitì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ fallback limit ê³„ì‚°

    Returns:
        int: Global fallback output limit (TPM), ë°ì´í„° ì—†ìœ¼ë©´ None
    """
    data = load_calibration_data()

    limits = []
    weights = []

    # ëª¨ë“  ì„¸ì…˜ ìœˆë„ìš°ì—ì„œ learned_output_limit ìˆ˜ì§‘ (weekly ì œì™¸)
    for window_key, window_data in data.items():
        if window_key == "weekly":
            continue

        model = window_data.get('model')
        if not model:
            continue

        learned_limit = model.get('learned_output_limit')
        sample_count = model.get('sample_count', 0)

        # Learned limitì´ ìˆê³  ìƒ˜í”Œì´ 3ê°œ ì´ìƒì¸ ê²½ìš°ë§Œ ì‚¬ìš©
        if learned_limit and learned_limit > 0 and sample_count >= 3:
            limits.append(learned_limit)
            # ìƒ˜í”Œ ìˆ˜ì— ë¹„ë¡€í•œ ê°€ì¤‘ì¹˜
            weight = min(sample_count / 10.0, 1.0)  # ìµœëŒ€ 1.0
            weights.append(weight)

    if not limits:
        return None

    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
    total_weight = sum(weights)
    weighted_avg = sum(l * w for l, w in zip(limits, weights)) / total_weight

    return round(weighted_avg)
```

#### 2. Fallback Logic ì¶”ê°€ (ìƒ˜í”Œ < 3ê°œ)

**íŒŒì¼**: `src/calibration_learner.py:566-604`

```python
if model['sample_count'] < 3:
    # ì¶©ë¶„í•œ ë°ì´í„° ì—†ìŒ - Global fallback limit ì‹œë„
    fallback_limit = get_global_fallback_limit()

    if fallback_limit and fallback_limit > 0 and window_key != "weekly":
        # Fallback limitìœ¼ë¡œ ê³„ì‚° (ë‹¤ë¥¸ ì„¸ì…˜ì˜ í•™ìŠµ ë°ì´í„° ì‚¬ìš©)
        try:
            output_file = Path.home() / '.claude_usage.json'
            with open(output_file, 'r') as f:
                usage_data = json.load(f)

            output_total = usage_data['session']['usage']['output_tokens']
            calibrated_pct = (output_total / (fallback_limit * 300)) * 100
            calibrated_value = calibrated_pct / 100.0

            return {
                'original_value': round(monitor_value, 4),
                'calibrated_value': round(calibrated_value, 4),
                'offset_applied': round(calibrated_value - monitor_value, 4),
                'confidence': 0.5,  # ì¤‘ê°„ ì‹ ë¢°ë„
                'status': 'learning_with_fallback',
                'threshold': BASELINE_THRESHOLD,
                'window_key': window_key,
                'method': 'fallback_limit',
                'fallback_limit': fallback_limit
            }
        except:
            pass  # Fallback ì‹¤íŒ¨ì‹œ ì›ë³¸ê°’ ì‚¬ìš©
```

#### 3. Fallback Logic ì¶”ê°€ (Limit Learning ì—†ìŒ)

**íŒŒì¼**: `src/calibration_learner.py:630-652`

```python
else:
    # Limit í•™ìŠµì´ ì—†ìŒ - Global fallback limit ì‹œë„
    fallback_limit = get_global_fallback_limit()

    if fallback_limit and fallback_limit > 0 and window_key != "weekly":
        # Fallback limitìœ¼ë¡œ ê³„ì‚° (ë‹¤ë¥¸ ì„¸ì…˜ì˜ í•™ìŠµ ë°ì´í„° ì‚¬ìš©)
        try:
            output_file = Path.home() / '.claude_usage.json'
            with open(output_file, 'r') as f:
                usage_data = json.load(f)

            output_total = usage_data['session']['usage']['output_tokens']
            calibrated_pct = (output_total / (fallback_limit * 300)) * 100
            calibrated_value = calibrated_pct / 100.0
            method = 'fallback_limit'
        except:
            # Fallback ì‹¤íŒ¨ì‹œ offset ë°©ì‹ìœ¼ë¡œ fallback
            calibrated_value = monitor_value + model['offset_mean']
            method = 'offset_based'
    else:
        # Fallbackë„ ì—†ìœ¼ë©´ offset ê¸°ë°˜ ì˜ˆì¸¡
        calibrated_value = monitor_value + model['offset_mean']
        method = 'offset_based'
        fallback_limit = None
```

**ê°œì„  íš¨ê³¼:**

```
Before (ìƒˆ ì„¸ì…˜):
â†’ Config limit ì‚¬ìš© (1,611 TPM)
â†’ ì‹¤ì œ 35%ì¸ë° 28% í‘œì‹œ âŒ

After (Global fallback):
â†’ 14:00-19:00 ì„¸ì…˜ì˜ learned_limit í™œìš© (1,293 TPM)
â†’ ì‹¤ì œ 35%, ëª¨ë‹ˆí„° 34.8% í‘œì‹œ âœ…
â†’ Status: 'learning_with_fallback'
â†’ Confidence: 0.5

After (3ê°œ ìƒ˜í”Œ ìˆ˜ì§‘):
â†’ ìì²´ learned_limit ì‚¬ìš©
â†’ Status: 'learning' â†’ 'learned'
â†’ Confidence: 0.7+
```

---

### Legacy limit_learner ì œê±° (2025-10-17)

**ë¬¸ì œ ìƒí™©:**
```
âš ï¸ Limit learning module not found. Using static limits.
```

ëª¨ë‹ˆí„° ì‹¤í–‰ ì‹œë§ˆë‹¤ warning ë©”ì‹œì§€ í‘œì‹œ. ì‹¤ì œë¡œëŠ” calibration_learner.pyë¥¼ ì‚¬ìš© ì¤‘ì´ë¯€ë¡œ ë¶ˆí•„ìš”í•œ ë©”ì‹œì§€.

**í•´ê²° ë°©ë²•:**

**íŒŒì¼**: `src/monitor_daemon.py`

**Removed**:
```python
# Lines 15-21 ì‚­ì œ
try:
    from limit_learner import record_session_snapshot, analyze_and_learn_limits, get_effective_limits
    LIMIT_LEARNING_ENABLED = True
except ImportError:
    LIMIT_LEARNING_ENABLED = False
    print("âš ï¸  Limit learning module not found. Using static limits.")
```

**Simplified**:
```python
# Lines 496-506: Configì—ì„œ ì§ì ‘ ë¡œë“œ
session_limits = config['rate_limits']['session']
weekly_limits = config['rate_limits']['weekly']
```

**Removed** (Lines 515-543):
- record_session_snapshot() í˜¸ì¶œ
- analyze_and_learn_limits() í˜¸ì¶œ
- LIMIT_LEARNING_ENABLED ì¡°ê±´ë¬¸
- limit_learning JSON ì¶œë ¥

**ê°œì„  íš¨ê³¼:**
```
Before:
âš ï¸ Limit learning module not found. Using static limits.
{
  "status": "active",
  "limit_learning": {...},
  ...
}

After:
{
  "status": "active",
  ...
}
âœ… ê¹”ë”í•œ ì¶œë ¥, warning ì—†ìŒ
```

---

**ê°œì„  ì™„ë£Œì¼**:
- Override ì‹¤ì‹œê°„ ê³„ì‚°: 2025-10-16
- Global Fallback Limit: 2025-10-17
- Legacy ëª¨ë“ˆ ì œê±°: 2025-10-17
- **v2.1 ì•ˆì •ì„± ê°•í™”: 2025-10-22** ğŸ†•

**ê´€ë ¨ íŒŒì¼**:
- `src/calibration_learner.py:205-243` (Global fallback í•¨ìˆ˜)
- `src/calibration_learner.py:401-450` (Override ì‹¤ì‹œê°„ ê³„ì‚°)
- `src/calibration_learner.py:566-652` (Fallback logic)
- `src/monitor_daemon.py` (Legacy ì½”ë“œ ì œê±°)
- `~/.local/bin/claude-usage-monitor` (ë°°í¬)
- `~/.local/bin/calibration_learner.py` (ë°°í¬)

---

## ğŸ†• v2.1 ì•ˆì •ì„± ê°•í™” (2025-10-22)

### ë¬¸ì œ ìƒí™©

**ë°˜ë³µë˜ëŠ” ì˜¤ë¥˜ë“¤:**
1. ì—¬ëŸ¬ daemon í”„ë¡œì„¸ìŠ¤ê°€ ë™ì‹œ ì‹¤í–‰ë˜ì–´ íŒŒì¼ ì¶©ëŒ
2. ê³¼ê±° ìœˆë„ìš°ì˜ overrideê°€ ìƒˆ ìœˆë„ìš°ì— ì˜í–¥
3. `claude-calibrate` ì‹¤í–‰ ì‹œ ì˜¤ë˜ëœ ë°ì´í„° ì‚¬ìš©
4. ë„ˆë¬´ ì‘ê±°ë‚˜ í° learned_limitìœ¼ë¡œ ì´ìƒí•œ ê°’ ê³„ì‚°
5. SwiftBarì—ì„œ calibration ìƒíƒœ íŒŒì•… ì–´ë ¤ì›€

### í•´ê²° ë°©ë²•

#### 1. PID íŒŒì¼ ê¸°ë°˜ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€

**íŒŒì¼**: `src/monitor_daemon.py`

**ì¶”ê°€ëœ í•¨ìˆ˜:**
```python
def check_pid():
    """PID íŒŒì¼ í™•ì¸ ë° ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€"""
    if PID_FILE.exists():
        try:
            with open(PID_FILE, 'r') as f:
                old_pid = int(f.read().strip())

            # í•´ë‹¹ PIDê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
            try:
                os.kill(old_pid, 0)  # ì‹œê·¸ë„ 0: í”„ë¡œì„¸ìŠ¤ ì¡´ì¬ í™•ì¸
                print(f"âš ï¸  Daemon already running with PID {old_pid}")
                return False
            except OSError:
                # í”„ë¡œì„¸ìŠ¤ê°€ ì—†ìœ¼ë©´ ì˜¤ë˜ëœ PID íŒŒì¼ ì‚­ì œ
                PID_FILE.unlink()
        except (ValueError, IOError):
            PID_FILE.unlink()

    return True

def write_pid():
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ PID ê¸°ë¡"""
    with open(PID_FILE, 'w') as f:
        f.write(str(os.getpid()))

def cleanup_pid():
    """PID íŒŒì¼ ì‚­ì œ"""
    if PID_FILE.exists():
        PID_FILE.unlink()
```

**ê°œì„  íš¨ê³¼:**
- ì¤‘ë³µ ì‹¤í–‰ ìë™ ê°ì§€ ë° ë°©ì§€
- ì˜¤ë˜ëœ PID íŒŒì¼ ìë™ ì •ë¦¬
- `--force` ì˜µì…˜ìœ¼ë¡œ ê°•ì œ ì‹œì‘ ê°€ëŠ¥

---

#### 2. ìœˆë„ìš° ê²€ì¦ ì‹œìŠ¤í…œ

**íŒŒì¼**: `src/calibration_learner.py:497-524`

**Before (ì‹œê°„ë§Œ ì²´í¬):**
```python
if datetime.now(ZoneInfo('Asia/Seoul')) < expires_at:
    # Override ì ìš©
```

**After (ìœˆë„ìš° + ì‹œê°„ ì²´í¬):**
```python
# Override ìœ íš¨ì„± ê²€ì¦
is_valid = False

# ì„¸ì…˜ ìœˆë„ìš°ì¸ ê²½ìš° - í˜„ì¬ ìœˆë„ìš°ì™€ ì¼ì¹˜í•´ì•¼ í•¨
if window_key != "weekly":
    current_window = get_session_window_key(now)
    if current_window == window_key and now < expires_at:
        is_valid = True
    elif current_window != window_key:
        # ë‹¤ë¥¸ ìœˆë„ìš°ì˜ override â†’ ì‚­ì œ
        del data[window_key]['latest_override']
        save_calibration_data(data)
else:
    # ì£¼ê°„ì€ ì‹œê°„ë§Œ ì²´í¬
    if now < expires_at:
        is_valid = True

# Overrideê°€ ìœ íš¨í•˜ë©´ ì ìš©
if is_valid:
    # ... override ë¡œì§
```

**ê°œì„  íš¨ê³¼:**
- ê³¼ê±° ìœˆë„ìš° override ìë™ ë§Œë£Œ
- ìœˆë„ìš° ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°©ì§€
- í˜„ì¬ ìœˆë„ìš°ë§Œ ì •í™•í•˜ê²Œ ì ìš©

---

#### 3. Pre-Calibration ì—…ë°ì´íŠ¸

**íŒŒì¼**: `~/.local/bin/claude-calibrate`

**Before:**
```bash
# Run calibration
python3 ~/.local/bin/calibration_learner.py "$@"
```

**After:**
```bash
# Force monitor update BEFORE calibration to get latest window data
if [ -n "$1" ] && [[ ! "$1" =~ ^-- ]]; then
    ~/.local/bin/claude-usage-monitor --once > /dev/null 2>&1
fi

# Run calibration
python3 ~/.local/bin/calibration_learner.py "$@"
```

**ê°œì„  íš¨ê³¼:**
- ìµœì‹  ìœˆë„ìš° ë°ì´í„°ë¡œ calibration
- ì˜¤ë˜ëœ ë°ì´í„° ì‚¬ìš© ë°©ì§€
- ì •í™•í•œ learned_limit ê³„ì‚°

---

#### 4. Learned Limit ë²”ìœ„ ê²€ì¦

**íŒŒì¼**: `src/calibration_learner.py`

**Constants ì¶”ê°€:**
```python
MIN_LEARNED_LIMIT = 100  # TPM ìµœì†Œê°’
MAX_LEARNED_LIMIT = 20000  # TPM ìµœëŒ€ê°’
```

**ê²€ì¦ ë¡œì§ ì¶”ê°€:**
```python
def reverse_calculate_limit(...):
    limit_rounded = round(limit_per_minute)

    # ë²”ìœ„ ê²€ì¦
    if limit_rounded < MIN_LEARNED_LIMIT:
        print(f"âš ï¸  Warning: Calculated limit ({limit_rounded} TPM) too low")
        return MIN_LEARNED_LIMIT
    elif limit_rounded > MAX_LEARNED_LIMIT:
        print(f"âš ï¸  Warning: Calculated limit ({limit_rounded} TPM) too high")
        return MAX_LEARNED_LIMIT

    return limit_rounded

# Override ì ìš© ì‹œì—ë„ ê²€ì¦
if learned_limit and learned_limit > 0:
    if learned_limit < MIN_LEARNED_LIMIT or learned_limit > MAX_LEARNED_LIMIT:
        # ë²”ìœ„ ë²—ì–´ë‚˜ë©´ fallbackë¡œ ì²˜ë¦¬
        learned_limit = None
```

**ê°œì„  íš¨ê³¼:**
- ê·¹ë‹¨ì ì¸ learned_limit ê°’ ë°©ì§€
- ìë™ ì¡°ì • ë° ê²½ê³ 
- ì•ˆì •ì ì¸ í¼ì„¼íŠ¸ ê³„ì‚°

---

#### 5. SwiftBar UI ê°œì„ 

**íŒŒì¼**: `plugins/ClaudeUsage.1m.sh`

**Before (ê¸°ë³¸ í‘œì‹œ):**
```bash
echo "ğŸ“š Calibration"
if [[ "$CALIB_STATUS" == "override" ]]; then
    echo "--Status: â­ Override (${CALIB_ADJUSTED}%)"
    echo "--Original: ${CALIB_ORIGINAL}%"
else
    echo "--Status: âš ï¸ No calibration"
fi
```

**After (ìƒì„¸ í‘œì‹œ):**
```bash
echo "ğŸ“š Calibration Status"
CALIB_WINDOW=$(jq -r '.calibration.session.window_key // "unknown"' "$USAGE_FILE")
CALIB_LIMIT=$(jq -r '.calibration.session.learned_limit // 0' "$USAGE_FILE")

if [[ "$CALIB_STATUS" == "override" ]]; then
    echo "--Session: â­ Override (${CALIB_ADJUSTED}%)"
    echo "--  Window: ${CALIB_WINDOW}"
    if [[ "$CALIB_LIMIT" != "0" ]] && [[ "$CALIB_LIMIT" != "null" ]]; then
        echo "--  Learned limit: ${CALIB_LIMIT} TPM"
    fi
    echo "--  Original: ${CALIB_ORIGINAL}%"
elif [[ "$CALIB_STATUS" == "calibrated" ]]; then
    echo "--Session: âœ… Calibrated (${CALIB_ADJUSTED}%)"
    echo "--  Window: ${CALIB_WINDOW}"
elif [[ "$CALIB_STATUS" == "learning" ]]; then
    echo "--Session: ğŸ“š Learning (${CALIB_ADJUSTED}%)"
    echo "--  Window: ${CALIB_WINDOW}"
else
    echo "--Session: âš ï¸ No calibration"
fi

# Weekly calibration status
WEEKLY_STATUS=$(jq -r '.calibration.weekly.status // "no_data"' "$USAGE_FILE")
WEEKLY_ADJUSTED=$(jq -r '.calibration.weekly.calibrated_percentage // 0' "$USAGE_FILE")
if [[ "$WEEKLY_STATUS" == "override" ]]; then
    echo "--Weekly: â­ Override (${WEEKLY_ADJUSTED}%)"
fi
```

**Monitor Daemon ì¶œë ¥ í¬í•¨:**
```python
session_calibration_info = {
    'original_percentage': session_percentages['max_percentage'],
    'calibrated_percentage': round(calibration['calibrated_value'] * 100, 1),
    'offset_applied': round(calibration['offset_applied'] * 100, 1),
    'confidence': calibration['confidence'],
    'status': calibration['status'],
    'dynamic_threshold': round(calibration['threshold'] * 100, 1),
    'window_key': window_key,
    'learned_limit': calibration.get('learned_limit')  # ì¶”ê°€
}
```

**ê°œì„  íš¨ê³¼:**
- í˜„ì¬ ìœˆë„ìš° í‘œì‹œë¡œ ë””ë²„ê¹… ìš©ì´
- Learned limit TPM ê°’ í™•ì¸ ê°€ëŠ¥
- ì„¸ì…˜/ì£¼ê°„ ìƒíƒœ ë¶„ë¦¬ í‘œì‹œ
- ì›ë³¸/Calibrated ë¹„êµ ê°€ëŠ¥

---

### í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] PID íŒŒì¼ ìƒì„± í™•ì¸
- [x] ì¤‘ë³µ ì‹¤í–‰ ì‹œ ê²½ê³  í‘œì‹œ
- [x] ìœˆë„ìš° ë³€ê²½ ì‹œ override ìë™ ì‚­ì œ
- [x] Calibration ì „ monitor ìë™ ì—…ë°ì´íŠ¸
- [x] Learned limit ë²”ìœ„ ê²€ì¦ (100~20,000 TPM)
- [x] SwiftBarì— ìƒì„¸ ì •ë³´ í‘œì‹œ

### ê°œì„  ì™„ë£Œì¼

- **2025-10-22**: v2.1 ì•ˆì •ì„± ê°•í™” ë¦´ë¦¬ìŠ¤

### ê´€ë ¨ íŒŒì¼

- `src/monitor_daemon.py`: PID íŒŒì¼ ì‹œìŠ¤í…œ
- `src/calibration_learner.py`: ìœˆë„ìš° ê²€ì¦, limit ê²€ì¦
- `~/.local/bin/claude-calibrate`: Pre-calibration ì—…ë°ì´íŠ¸
- `plugins/ClaudeUsage.1m.sh`: UI ê°œì„ 
- `CHANGELOG.md`: ë²„ì „ë³„ ë³€ê²½ì‚¬í•­
